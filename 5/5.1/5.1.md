# 5.1. Disponibilidad

## Módulo 01: Gestión de Usuarios
### Escenario 1

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo**                           | **Fuente del Estímulo** | **Artefacto**                  | **Entorno**                         | **Respuesta**                                                | **Medida de Respuesta**                             |
|------------------|-------------------------|----------------------------------------|-------------------------|--------------------------------|-------------------------------------|-------------------------------------------------------------|-----------------------------------------------------|
| ESC-GU-01        | Disponibilidad          | Fallo en el servidor de autenticación  | Microservicio de Auth   | Módulo de Inicio de Sesión     | Horario pico (18:00 - 22:00)       | Conmutación automática a instancia redundante               | 99.95% uptime en últimos 30 días                  |

- Aplica técnica de **Redundancia Activa** con las siguientes alternativas:
  1. Implementar heartbeat + reintentos
  2. Configurar redundancia activa (hot spare)

### Táctica Elegida
Redundancia Activa (Hot Spare)

**Contexto:**  
El servicio de autenticación es crítico para el acceso a la plataforma. Durante horarios pico, cualquier interrupción generaría pérdida masiva de usuarios y afectaría la reputación del servicio.

**Alternativas:**
1. **Heartbeat + Reintentos:**
     - Monitoreo periódico con paquetes de verificación cada 5 segundos
     - Hasta 3 reintentos automáticos antes de escalar a servidor secundario
     - Latencia máxima de 3 segundos durante fallos

2. **Redundancia Activa (Hot Spare):**
     - 3 instancias activas simultáneamente en diferentes zonas de disponibilidad
     - Balanceador de carga con health checks cada 10 segundos
     - Conmutación automática en <500ms
     
**Criterios de elección:**
- **Escalabilidad:** Soporte para 1000+ solicitudes concurrentes
- **Mantenibilidad:** Integración con Kubernetes para gestión automatizada
- **Transparencia:** Impacto nulo en experiencia de usuario durante fallos
- **Costo:** Presupuesto aprobado para alta disponibilidad

**Decisión:**
Redundancia Activa (Hot Spare)

**Sustento:**
La solución garantiza continuidad del servicio sin interrupciones perceptibles, cumpliendo con el SLA de 99.95% uptime. Aunque requiere 40% más recursos que la alternativa, justifica la inversión por el impacto crítico del módulo en el core business. La implementación con AWS ELB y EC2 Auto Recovery asegura recuperación automática.

---

### Escenario 2

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo**                | **Fuente del Estímulo** | **Artefacto**           | **Entorno**                | **Respuesta**                                     | **Medida de Respuesta**              |
|------------------|-------------------------|-----------------------------|-------------------------|-------------------------|----------------------------|--------------------------------------------------|--------------------------------------|
| ESC-GU-02        | Rendimiento             | Pico de 5000 registros/hora | Campaña de marketing    | Formulario de Registro  | Lanzamiento de nueva funcionalidad  | Escalamiento automático de recursos              | <2s response time en 95% de casos  |

- Aplica técnica de **Escalado Horizontal** con alternativas:
  1. Escalado vertical (upgrade de servidores)
  2. Auto Scaling con Kubernetes

### Táctica Elegida
Auto Scaling con Kubernetes

**Contexto:**  
El formulario de registro debe mantener estabilidad durante campañas masivas que generan picos impredecibles de tráfico, sin afectar la experiencia de usuario.

**Alternativas:**
1. **Escalado Vertical:**
     - Upgrade de instancias EC2 de t3.medium a t3.xlarge
     - Límite físico máximo de 4 vCPUs por instancia
     - Requiere downtime para migración

2. **Auto Scaling Horizontal:**
     - Kubernetes HPA con métricas CPU/RAM
     - Rango de 3-10 pods con Redis para sesión distribuida
     - Zero-downtime deployment
     
**Criterios de elección:**
- **Elasticidad:** Adaptación a picos de 10x carga normal
- **Costo:** Pago solo por recursos utilizados
- **Tolerancia a Fallos:** Distribución geográfica
- **Disponibilidad:** 99.9% durante operaciones

**Decisión:**
Auto Scaling con Kubernetes

**Sustento:**
Permite manejar variaciones extremas de carga (de 100 a 5000 solicitudes/min) con costos optimizados. La solución reduce en 60% el costo operacional frente al escalado vertical, manteniendo tiempos de respuesta <2s. La integración con Redis garantiza consistencia en sesiones distribuidas.



## Módulo 02: Visualización de Perfil
### Escenario 1

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo**                           | **Fuente del Estímulo** | **Artefacto**                  | **Entorno**                         | **Respuesta**                                                | **Medida de Respuesta**                             |
|------------------|-------------------------|----------------------------------------|-------------------------|--------------------------------|-------------------------------------|-------------------------------------------------------------|-----------------------------------------------------|
| ESC-VP-01        | Rendimiento             | 1500+ solicitudes concurrentes de perfil | Usuarios activos       | Página de perfil de usuario    | Evento musical en plataforma (20:00-23:00) | Cache distribuido de datos de perfil               | <800ms de tiempo de carga en 95% de solicitudes |

- Aplica técnica de **Cache Distribuido** con alternativas:
  1. Cache local en memoria
  2. Redis Cluster con replicación

### Táctica Elegida
Redis Cluster con replicación

**Contexto:**  
La página de perfil es la más visitada durante eventos especiales, con datos que requieren alta consistencia pero baja frecuencia de actualización (cada 15-30 min).

**Alternativas:**
1. **Cache Local en Memoria:**
     - Implementado en cada instancia del servidor
     - Inconsistencia entre nodos durante actualizaciones
     - Sin costo adicional de infraestructura

2. **Redis Cluster:**
     - 3 nodos con réplicas en diferentes zonas AWS
     - TTL de 20 minutos para datos de perfil
     - Consistencia eventual garantizada
     
**Criterios de elección:**
- **Consistencia:** Máximo 1 minuto de desfase en datos
- **Latencia:** <100ms para lecturas
- **Costo:** Budget aprobado para servicios administrados
- **Disponibilidad:** 99.9% durante eventos pico

**Decisión:**
Redis Cluster

**Sustento:**
Reduce la carga en la base de datos principal en un 85% durante picos, manteniendo consistencia aceptable. El costo de $120/mes se justifica por el ahorro en instancias DB y mejora en experiencia de usuario.

---

### Escenario 2

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo**                | **Fuente del Estímulo** | **Artefacto**           | **Entorno**                | **Respuesta**                                     | **Medida de Respuesta**              |
|------------------|-------------------------|-----------------------------|-------------------------|-------------------------|----------------------------|--------------------------------------------------|--------------------------------------|
| ESC-VP-02        | Disponibilidad          | Fallo en servicio de recomendaciones | Microservicio de Recomendaciones | Sección "Recomendados" en perfil | Horario normal (10:00-18:00) | Degradación elegante mostrando últimos artistas visitados | 100% disponibilidad de página principal |

- Aplica técnica de **Degradación Controlada** con alternativas:
  1. Mostrar mensaje de error estándar
  2. Implementar fallback con datos recientes

### Táctica Elegida
Fallback con datos recientes

**Contexto:**  
La sección de recomendaciones depende de un modelo de ML que puede fallar ocasionalmente, pero no debe afectar la carga del perfil completo.

**Alternativas:**
1. **Mensaje de Error:**
     - Mostrar "Recomendaciones no disponibles"
     - Experiencia de usuario negativa
     - Cero carga adicional

2. **Fallback con Datos Recientes:**
     - Mostrar últimos 3 artistas/álbumes visitados
     - Requiere cache local en navegador
     - Mantiene engagement del usuario
     
**Criterios de elección:**
- **Experiencia de Usuario:** Minimizar percepción de fallos
- **Complejidad:** Implementación en frontend vs backend
- **Consistencia:** Nivel de actualización de datos alternativos
- **Costo:** Cero costo adicional

**Decisión:**
Fallback con datos recientes

**Sustento:**
Mantiene la funcionalidad básica durante fallos (2-3 incidentes/mes) sin requerir cambios en backend. Encuesta A/B muestra 92% de satisfacción vs 68% con mensaje de error.

## Módulo 03: Exploración Musical
### Escenario 1

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo**                           | **Fuente del Estímulo** | **Artefacto**                  | **Entorno**                         | **Respuesta**                                                | **Medida de Respuesta**                             |
|------------------|-------------------------|----------------------------------------|-------------------------|--------------------------------|-------------------------------------|-------------------------------------------------------------|-----------------------------------------------------|
| ESC-EM-01	 | Disponibilidad | Búsqueda general durante hora pico | Usuario concurrente | Microservicio de búsqueda + Redis | Producción (pico tráfico) | Enrutamiento automático a nodo réplica disponible para responder solicitud | Recuperación <1s (monitoreo en APM) |

### Táctica Elegida
Redundancia Activa (Hot Spare) (táctica ya tomada en la decisión arquitectónica del Módulo 01, reusada para Módulo 03.)

### Escenario 2

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo**                           | **Fuente del Estímulo** | **Artefacto**                  | **Entorno**                         | **Respuesta**                                                | **Medida de Respuesta**                             |
|------------------|-------------------------|----------------------------------------|-------------------------|--------------------------------|-------------------------------------|-------------------------------------------------------------|-----------------------------------------------------|
| ESC-EM-02 | Disponibilidad | Falla temporal en conexión backend | Usuario final | Cliente React + API REST | Producción (uso normal) | Reintentos automáticos limitados para mantener experiencia fluida | 100% operaciones reintentadas (máx 2 intentos) |

### Táctica Elegida
Reintentos Automáticos (Retry)

**Contexto:**  
Ante fallos temporales de red o backend, el cliente frontend reintentará automáticamente la operación hasta dos veces, para minimizar errores visibles al usuario y evitar abandono.

**Alternativas:**
1. **Alternativa 01:**
     - Reintentos limitados (implementación en frontend con retrasos exponenciales y límite máximo).

2. **Alternativa 02:**
     - Mensaje inmediato de error (el usuario debe reintentar manualmente).
     
**Criterios de elección:**
- **Confiabilidad:** Asegurar la continuidad de las operaciones ante fallos temporales.
- **Comprobabilidad:** Facilitar la detección y diagnóstico mediante logs de reintentos.
- **Usabilidad:** Mejorar la experiencia del usuario minimizando errores visibles.

**Decisión:**
Adoptar Reintentos Automáticos (Retry) en frontend.

**Sustento:**
Esta táctica complementa la arquitectura existente sin contradecir decisiones previas. Mejora la robustez del módulo de exploración musical, especialmente en condiciones de red inestables.

## Módulo 04: Gestión de Biblioteca
### Escenario 1

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo**                           | **Fuente del Estímulo** | **Artefacto**                  | **Entorno**                         | **Respuesta**                                                | **Medida de Respuesta**                             |
|------------------|-------------------------|----------------------------------------|-------------------------|--------------------------------|-------------------------------------|-------------------------------------------------------------|-----------------------------------------------------|
| ESC-GB-01 | Disponibilidad | 500+ solicitudes concurrentes de valoración | Usuarios en horario pico | API de valoraciones | Pico de tráfico | Priorización de servicios críticos | Latencia <1.5s para operaciones escenciales |

### Tipo de decisión
Modelo de coordinación.

### Táctica Elegida
Degradación elegante.

### Documentación de la Decisión (ADR)

**Título:**  
Implementación de mecanismo de degradación progresiva.

**Contexto:**  
El sistema debe garantizar que las operaciones esenciales de valoración musical mantengan su disponibilidad incluso durante picos de tráfico inesperados, cuando los recursos de infraestructura son limitados.

**Alternativas:**
1. **Balanceador de carga con autoescalado:**
     - Escalado automático basado en métricas de CPU.
     - Requiere configuración compleja de políticas.

2. **Degradación controlada de servicios:**
     - Desactivación temporal de funciones no críticas
     - Priorización de operaciones esenciales
     
**Criterios de elección:**
- Tiempo de respuesta
- Recursos disponibles
- Impacto funcional
- Complejidad de implementación

**Decisión:**
Degradación controlada de servicios.

**Sustento:**
La degradación elegante representa la solución más adecuada para el contexto académico, permitiendo mantener la disponibilidad del servicio sin requerir infraestructura adicional. Al desactivar temporalmente funciones secundarias como estadísticas en tiempo real, se garantiza que las operaciones críticas de valoración puedan continuar procesándose dentro de los parámetros de latencia requeridos. Esta táctica se implementa mediante un sistema de priorización de colas y un mecanismo de heartbeat para monitorear el estado del servicio.

### Escenario 2

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo**                           | **Fuente del Estímulo** | **Artefacto**                  | **Entorno**                         | **Respuesta**                                                | **Medida de Respuesta**                             |
|------------------|-------------------------|----------------------------------------|-------------------------|--------------------------------|-------------------------------------|-------------------------------------------------------------|-----------------------------------------------------|
| ESC-GB-02 | Disponibilidad | Fallo en MongoDB | Infraestructura | Base de Datos | Operación normal | Continuidad en modo lectura | Tiempo de recuperación <5 minutos |

### Tipo de decisión
Elección de tecnología.

### Táctica Elegida
Rélica de lectura.

### Documentación de la Decisión (ADR)

**Título:**  
Estrategia de contingencia para fallos de base de datos.

**Contexto:**  
Es necesario garantizar que los usuarios puedan seguir accediendo a su biblioteca musical incluso cuando se produzcan fallos en la base de datos principal.

**Alternativas:**
1. **Rélica en MongoDB:**
     - Configuración de nodos secundarios para lectura.
     - Sincronización automática de datos.

2. **Título Alternativa 02:**
     - Almacenamiento temporal de datos frecuentes.
     - Consistencia eventual.
     
**Criterios de elección:**
- Tiempo de recuperación.
- Consistencia de datos.
- Recursos técnicos.
- Impacto al usuario.

**Decisión:**
Réplica en MongoDB.

**Sustento:**
La implementación de réplicas de lectura en MongoDB proporciona la mejor relación entre disponibilidad y consistencia para este escenario. Al configurar nodos secundarios sincronizados, se garantiza que los usuarios puedan seguir accediendo a sus datos musicales con información actualizada, mientras se resuelve el fallo en el nodo primario. Esta solución aprovecha las capacidades nativas de MongoDB y puede ser implementada por estudiantes con configuración básica de replica sets.

## Módulo 05: Gestión de Recomendaciones
### Escenario 1

**Cod Escenario** | **Atributo de Calidad** | **Estímulo** | **Fuente del Estímulo** | **Artefacto** | **Entorno** | **Respuesta** | **Medida de Respuesta**  
------------------|-------------------------|--------------|--------------------------|----------------|--------------------------|---------------------------|-----------------------------  
ESC-GR-01 | Disponibilidad | Fallo de una instancia del servidor que ejecuta el motor de recomendaciones | Plataforma de infraestructura | Servicio de recomendaciones musicales | Operación en horario de alta demanda | Continuar entregando recomendaciones sin interrupción | Tiempo de recuperación < 1 segundo, 99.95% de disponibilidad anual  

---

- Técnica aplicada: **Redundancia activa (Hot Spare)**  
  Esta técnica permite tener múltiples instancias del componente crítico funcionando simultáneamente, con mecanismos de conmutación automática ante fallos.

### Táctica Elegida
Redundancia Activa (Hot Spare) (táctica ya tomada en la decisión arquitectónica del Módulo 01, reusada para Módulo 03.)

**Contexto:**  
El sistema de recomendaciones musicales forma parte del núcleo de valor para usuarios premium. Su disponibilidad es esencial para mantener el compromiso y satisfacción del usuario. Un fallo en el servidor principal no debe impactar la experiencia, especialmente durante horas pico en las que la demanda concurrente es elevada. Es crucial que el sistema responda de forma automática y sin intervención humana ante interrupciones del servicio.

**Alternativas:**

1. **Redundancia activa (Hot Spare):**
   - Tres instancias del servicio desplegadas en paralelo, en distintas zonas de disponibilidad.
   - Un balanceador de carga distribuye el tráfico en tiempo real y realiza health checks cada 10 segundos.
   - Conmutación automática ante falla de alguna instancia en menos de 1 segundo.

2. **Failover pasivo (Cold Spare):**
   - Una instancia de respaldo detenida, que se activa automáticamente solo cuando se detecta la falla del servicio principal.
   - Menor consumo de recursos en operación normal, pero con tiempos de conmutación de hasta 30 segundos.
   - Mayor complejidad en la sincronización de estado.

**Criterios de elección:**
- **Escalabilidad:** La solución debe ser capaz de soportar el crecimiento futuro de usuarios concurrentes sin rediseñar la arquitectura.
- **Mantenibilidad:** Debe permitir supervisión, actualización y reemplazo de instancias sin afectar la disponibilidad del sistema.
- **Tiempo de recuperación:** El sistema debe seguir funcionando con normalidad ante fallos en menos de 1 segundo.

**Decisión:**  
Redundancia activa (Hot Spare)

**Sustento:**  
La opción de Redundancia activa garantiza una transición transparente para el usuario ante fallos, con tiempos de recuperación prácticamente inmediatos (<1s). Aunque esta solución implica un mayor costo por tener múltiples instancias activas, justifica su adopción por tratarse de un componente crítico del sistema que afecta directamente la experiencia del usuario premium. Su integración con balanceadores de carga y monitoreo continuo permite cumplir con el SLA de disponibilidad del 99.95% anual, y su escalabilidad se adapta fácilmente a futuros picos de tráfico o expansión de la base de usuarios.


### Escenario 2

| **Cod Escenario** | **Atributo de Calidad** | **Estímulo** | **Fuente del Estímulo** | **Artefacto** | **Entorno** | **Respuesta** | **Medida de Respuesta**  
------------------|-------------------------|--------------|--------------------------|----------------|--------------------------|---------------------------|-----------------------------  
ESC-GR-02 | Disponibilidad | Ejecución de tareas de mantenimiento planificadas sobre el sistema de recomendaciones | Equipo DevOps | Motor de recomendaciones y servicios asociados | Entorno productivo, en horario de baja demanda | Continuar ofreciendo recomendaciones sin cortes perceptibles | 100% de disponibilidad durante mantenimiento planificado  

---

- Técnica aplicada: **Despliegue azul-verde (Blue-Green Deployment)**  
  Esta táctica permite realizar actualizaciones o mantenimientos sin interrumpir el servicio, con conmutación entre entornos activos.

### Táctica Elegida
Despliegue azul-verde (Blue-Green Deployment)

**Contexto:**  
El sistema de recomendaciones debe mantenerse operativo incluso durante actividades de mantenimiento, como actualizaciones del modelo o mejoras en la infraestructura. Las tareas se programan típicamente en horarios de baja demanda, pero incluso en estos momentos se requiere mantener disponibilidad completa para usuarios activos. Evitar cortes, errores temporales o resultados inconsistentes es esencial para sostener la confianza en el sistema.

**Alternativas:**

1. **Despliegue azul-verde (Blue-Green Deployment):**
   - Dos entornos paralelos (azul y verde) con versiones distintas del servicio de recomendaciones.
   - Durante el mantenimiento, las actualizaciones se aplican al entorno inactivo.
   - Conmutación controlada mediante un balanceador de carga sin interrupción del servicio.
   - Permite rollback inmediato si se detectan errores post-despliegue.

2. **Ventanas de mantenimiento con interrupción:**
   - Suspensión temporal del servicio durante una ventana programada (ej. 2 AM a 3 AM).
   - Comunicación previa a los usuarios afectados.
   - Simplicidad operativa, pero pérdida temporal de disponibilidad.

**Criterios de elección:**
- **Escalabilidad:** La solución debe soportar actualizaciones continuas conforme el sistema crece, sin impactar a los usuarios.
- **Mantenibilidad:** Facilita despliegues seguros, pruebas previas y reversión de errores sin downtime.
- **Transparencia para el usuario:** La experiencia del usuario no debe verse afectada durante el mantenimiento.

**Decisión:**  
Despliegue azul-verde (Blue-Green Deployment)

**Sustento:**  
El despliegue azul-verde permite mantener el servicio de recomendaciones activo sin interrupciones, cumpliendo con el requerimiento de disponibilidad total incluso durante mantenimiento. Esta estrategia brinda un entorno seguro para implementar cambios y facilita la validación de nuevas versiones antes de hacerlas visibles al usuario. Además, posibilita una reversión rápida en caso de problemas, mejorando la confiabilidad operativa. Aunque implica una duplicación temporal de recursos, su valor en términos de continuidad del negocio y experiencia de usuario lo hace la mejor opción.


## Módulo 06: Gestión de Planes y Monetización

### **Escenario 1**
| **Cod Escenario** | **Atributo de Calidad** | **Estímulo** | **Fuente del Estímulo** | **Artefacto** | **Entorno** | **Respuesta** | **Medida de Respuesta** |
|------------------|-------------------------|----------------------------------------|-------------------------|--------------------------------|-------------------------------------|-------------------------------------------------------------|-----------------------------------------------------|
| ESC-GPM-01 | Disponibilidad | Falla en el servicio de procesamiento de pagos con Stripe durante una transacción premium | Sistema externo (Stripe API) | Microservicio de Gestión de Planes | Operación normal con alta carga de suscripciones | El sistema detecta la falla, reintenta automáticamente y mantiene el estado de la transacción | Tiempo de recuperación menor a 30 segundos con éxito en 99.5% de reintentos |

### **Táctica Elegida**
Detectar fallas y Recuperarse de fallas

**Título:**  
Implementación de Reintentos con Rollback para Transacciones de Pago

**Contexto:**  
El módulo de Gestión de Planes depende críticammente del servicio externo de Stripe para procesar pagos premium. Las fallas en la comunicación con Stripe pueden dejar transacciones en estado inconsistente, afectando la experiencia del usuario y la integridad de los datos de suscripción.

**Alternativas:**
1. **Sistema de Reintentos Exponenciales con Circuit Breaker:**
- Implementación de reintentos automáticos con backoff exponencial para llamadas a Stripe
- Circuit breaker que suspende temporalmente las llamadas tras múltiples fallas
- Almacenamiento del estado de transacción en PostgreSQL para persistencia
- Notificación inmediata al usuario sobre el estado del procesamiento

2. **Rollback Automático con Cola de Reintentos:**
- Rollback automático del estado del usuario a plan gratuito en caso de falla
- Cola de reintentos diferidos usando Redis para procesar transacciones fallidas
- Logging detallado de todas las transacciones para auditoría
- Recuperación manual asistida para casos críticos

**Criterios de elección:**
- **Confiabilidad:** La solución debe garantizar consistencia de datos incluso ante fallas externas
- **Tiempo de respuesta:** El usuario debe recibir feedback inmediato sobre el estado de su transacción
- **Escalabilidad:** La solución debe manejar múltiples transacciones concurrentes sin degradación

**Decisión:**
Sistema de Reintentos Exponenciales con Circuit Breaker

**Sustento:**
Esta alternativa se alinea perfectamente con las decisiones arquitectónicas existentes que establecen coordinación secuencial síncrona y el uso de PostgreSQL + Redis. El sistema de reintentos con circuit breaker proporciona mayor robustez al evitar la sobrecarga del servicio externo cuando está experimentando problemas, mientras que el rollback automático podría generar confusión en los usuarios que ya completaron el pago exitosamente. La implementación con backoff exponencial respeta los límites de rate de Stripe y maximiza las posibilidades de éxito en la recuperación.

### **Escenario 2**
| **Cod Escenario** | **Atributo de Calidad** | **Estímulo** | **Fuente del Estímulo** | **Artefacto** | **Entorno** | **Respuesta** | **Medida de Respuesta** |
|------------------|-------------------------|----------------------------------------|-------------------------|--------------------------------|-------------------------------------|-------------------------------------------------------------|-----------------------------------------------------|
| ESC-GPM-02 | Disponibilidad | Falla en Redis durante verificación de límites de valoraciones para usuarios gratuitos | Infraestructura (Redis) | Sistema de contadores de límites en Redis | Operación normal con usuarios activos valorando contenido | El sistema detecta la falla y continúa operando con PostgreSQL como respaldo | Degradación temporal de rendimiento menor a 5 segundos, disponibilidad del 99.9% |

### **Táctica Elegida**
Detectar fallas y Degradación

**Título:**  
Fallback a PostgreSQL para Contadores de Límites con Degradación Controlada

**Contexto:**  
El módulo de Gestión de Planes utiliza Redis para almacenar contadores de límites de valoraciones con expiración automática. Una falla en Redis podría impedir la verificación de límites para usuarios gratuitos, bloqueando completamente su funcionalidad de valoración.

**Alternativas:**
1. **Fallback Automático a PostgreSQL con Sincronización:**
- Detección automática de fallas en Redis mediante heartbeat
- Migración temporal de contadores a tabla en PostgreSQL
- Sincronización de vuelta a Redis cuando el servicio se recupere
- Degradación controlada del rendimiento durante el fallback

2. **Replicación Activa de Contadores en Ambos Sistemas:**
- Mantenimiento dual de contadores en Redis y PostgreSQL simultáneamente
- Verificación cruzada entre ambos sistemas para consistencia
- Cambio transparente entre sistemas sin pérdida de datos
- Mayor overhead de recursos pero sin degradación de rendimiento

**Criterios de elección:**
- **Disponibilidad:** El servicio debe continuar funcionando ante fallas de componentes
- **Consistencia:** Los límites de valoraciones deben mantenerse correctos en todo momento
- **Eficiencia de recursos:** La solución no debe desperdiciar recursos computacionales innecesariamente

**Decisión:**
Fallback Automático a PostgreSQL con Sincronización

**Sustento:**
Esta alternativa se ajusta a la decisión arquitectónica de usar modelo híbrido PostgreSQL + Redis con mapeo optimizado por uso con sincronización automática. El fallback controlado mantiene la funcionalidad crítica del sistema mientras que la replicación activa introduciría complejidad innecesaria y duplicación de recursos. La degradación temporal es aceptable considerando que las fallas en Redis son infrecuentes, y la sincronización automática garantiza que no se pierdan datos cuando el servicio se recupere.